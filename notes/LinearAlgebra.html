<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <title>LinearAlgebra | Jasper Wang</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    <script type="text/javascript">
  window.onload = function() {
    document.getElementsByClassName("status-banner")[0].style.display = "block";
    setTimeout(function() {
      renderMathElements(document.getElementsByClassName("math"));
      document.getElementsByClassName("status-banner")[0].style.display = "none";
    }, 50); // delay to allow status banner to show
  }

  function renderMathElements(mathElements) {
    var mathOptions = {
      macros: {
        "\\set": "\\left\\{ #1 \\right\\}",
        "\\tup": "\\left\\langle #1 \\right\\rangle",
        "\\abs": "\\left\\lvert #1 \\right\\rvert",
        "\\floor": "\\left\\lfloor #1 \\right\\rfloor",
        "\\ceil": "\\left\\lceil#1 \\right\\rceil",
        "\\mb": "\\mathbb{#1}",
        "\\rem": "\\operatorname{rem}",
        "\\ord": "\\operatorname{ord}",
        "\\sign": "\\operatorname{sign}",
        "\\imag": "\\bm{i}",
        "\\dee": "\\mathop{}\\!\\mathrm{d}",
        "\\lH": "\\overset{\\text{l'H}}{=}",
        "\\evalat": "\\left.\\left(#1\\right)\\right|",
        "\\sech": "\\operatorname{sech}",
        "\\spn": "\\operatorname{Span}",
        "\\proj": "\\operatorname{proj}",
        "\\prp": "\\operatorname{perp}",
        "\\refl": "\\operatorname{refl}",
        "\\magn": "\\left\\lVert #1 \\right\\rVert",
        "\\rank": "\\operatorname{rank}",
        "\\sys": "\\left[ #1 \\mid #2\\space \\right]",
        "\\range": "\\operatorname{Range}",
        "\\adj": "\\operatorname{adj}",
        "\\cof": "\\operatorname{cof}",
        "\\coord": "{\\left\\lbrack #1 \\right\\rbrack}_{#2}",
        "\\diag": "\\operatorname{diag}",
        "\\formlp": "\\operatorname{Form}(\\mathcal{L}^P)",

        // not yet available in KaTeX
        "\\operatorname": "\\mathop{\\text{#1}}\\nolimits", //wip: spacing is slightly off
        "\\not": "\\rlap{\\kern{7.5mu}/}", //wip: slash angle is slightly off
        "\\bm": "\\mathbf", //wip: should be italic, but isn't
      },
      throwOnError: false,
    };
    for (var i=0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      katex.render(texText.data, mathElements[i], mathOptions);
    }
  }
  </script>
</head>
<body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-68271407-1', 'auto');
    ga('send', 'pageview');

  </script>
<div id="TOC">
<ul>
<li><a href="#notations">Notations</a><ul>
<li><a href="#point">Point</a></li>
<li><a href="#vector">Vector</a></li>
<li><a href="#matrix">Matrix</a></li>
<li><a href="#tensor">Tensor</a></li>
</ul></li>
<li><a href="#vector-1">Vector</a><ul>
<li><a href="#basics">Basics</a><ul>
<li><a href="#length">Length</a></li>
</ul></li>
<li><a href="#operations">Operations</a><ul>
<li><a href="#vector-addition">Vector addition</a></li>
<li><a href="#scalar-multiply">Scalar Multiply</a></li>
<li><a href="#dot-product">Dot product</a></li>
<li><a href="#cross-product">Cross Product</a></li>
</ul></li>
<li><a href="#linear-combinations">Linear Combinations</a><ul>
<li><a href="#linearly-independence">Linearly Independence</a></li>
</ul></li>
<li><a href="#orthogonal-perpendicular">Orthogonal (Perpendicular)</a></li>
<li><a href="#projection">Projection</a><ul>
<li><a href="#height-distance">Height (distance)</a></li>
</ul></li>
<li><a href="#line">Line</a></li>
<li><a href="#plane">Plane</a><ul>
<li><a href="#intersection-between-a-line-and-a-plane">Intersection between a line and a plane</a></li>
<li><a href="#angle-of-intersection-between-2-planes">Angle of intersection between 2 planes</a></li>
<li><a href="#hyperplane">Hyperplane</a></li>
</ul></li>
<li><a href="#unit-vector">Unit vector</a></li>
</ul></li>
<li><a href="#matrix-1">Matrix</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#special-matrices">Special Matrices</a></li>
<li><a href="#operations-1">Operations</a><ul>
<li><a href="#element-wise-product-hadamard-product">Element-wise product (Hadamard product)</a></li>
<li><a href="#matrices-multiplication">Matrices Multiplication</a></li>
</ul></li>
<li><a href="#matrix-inverse">Matrix Inverse</a></li>
<li><a href="#determinant">Determinant</a></li>
<li><a href="#system-of-linear-equations">System of linear equations</a><ul>
<li><a href="#rref">RREF</a></li>
<li><a href="#gaussian-elimination">Gaussian Elimination</a></li>
<li><a href="#linear-dependence-and-span">Linear dependence and Span</a></li>
<li><a href="#goal-solve-avec-x-vec-b-a-in-renn-a-is-invertible">Goal: solve <span class="math display">\displaystyle  A\vec x = \vec b, A \in \Re^{n*n} </span>, A is invertible</a></li>
<li><a href="#thm-lu-factorization">Thm (LU factorization)</a></li>
<li><a href="#to-solve-linear-system-ax-b">To solve linear system Ax = b</a></li>
</ul></li>
<li><a href="#eigen-decomposition">Eigen Decomposition</a></li>
<li><a href="#singular-value-decomposition">Singular Value Decomposition</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>
<h1 id="notations">Notations</h1>
<h2 id="point">Point</h2>
<ul>
<li>Points are represented via <span class="math display">\displaystyle ()</span>
<ul>
<li>e.g. <span class="math display">\displaystyle p_1 = (x_1, y_1) </span> (2D), <span class="math display">\displaystyle p_2 = (x_2, y_2, z_2) </span></li>
</ul></li>
</ul>
<h2 id="vector">Vector</h2>
<ul>
<li>Scalar: <span class="math display">\displaystyle  s \in \Re </span></li>
<li>Vectors are represented via <span class="math display">\displaystyle &lt;&gt;</span> as a row vector or in column form <span class="math display">\displaystyle []</span> as a column vector with name <span class="math display">\displaystyle \vec v</span>
<ul>
<li><span class="math display">\displaystyle  \vec v \in \Re^n </span> i.e. it has n elements</li>
<li>Each element in <span class="math display">\displaystyle \vec v</span> is called a component, an entry, or a coordinate, and it's denoted as <span class="math display">\displaystyle v_i</span> for ith element where 0 &lt; i &lt;= n</li>
<li>e.g. <span class="math display">\displaystyle  \vec v = &lt;1, -2, 3&gt; \equiv \vec v = i - 2j + 3k \equiv \vec v = \hat v - 2\hat j + 3\hat k </span>. In this case <span class="math display">\displaystyle v_1 = 1, v_2 = -2</span></li>
</ul></li>
<li>Unit vectors are represented as <span class="math display">\displaystyle \hat v </span></li>
<li>Length would be denoted as <span class="math display">\displaystyle |\vec v|</span></li>
</ul>
<h2 id="matrix">Matrix</h2>
<ul>
<li><span class="math display">\displaystyle  A \in \Re^{m*n} </span> where m is the number of rows, and n is the number of columns</li>
<li>Each element: <span class="math display">\displaystyle  A_{i,j} </span>, ith row: <span class="math display">\displaystyle  A_{i,:} </span>, jth column: <span class="math display">\displaystyle  A_{:,j} </span></li>
</ul>
<h2 id="tensor">Tensor</h2>
<ul>
<li>Tensor: array with more than 2 axes (3D comparing to matrix which is 2-D)
<ul>
<li>To get specific element: <span class="math display">\displaystyle  A_{i,j,k} </span></li>
</ul></li>
</ul>
<h1 id="vector-1">Vector</h1>
<h2 id="basics">Basics</h2>
<ul>
<li>Each entry / component <span class="math display">\displaystyle v_i</span> in <span class="math display">\displaystyle \vec v</span> is a real number scalar i.e. <span class="math display">\displaystyle v_i \in \Re </span></li>
<li><span class="math display">\displaystyle  x_S </span> where S is {1,3,6} represents {<span class="math display">\displaystyle  x_1, x_3, x_6 </span>}</li>
</ul>
<h3 id="length">Length</h3>
<h2 id="operations">Operations</h2>
<h3 id="vector-addition">Vector addition</h3>
<ul>
<li><span class="math display">\displaystyle \vec v + \vec u = &lt;v_1 + u_1, v_2 + u_2, ...&gt; </span></li>
</ul>
<h3 id="scalar-multiply">Scalar Multiply</h3>
<ul>
<li><span class="math display">\displaystyle c\vec v = &lt;cv_1, cv_2, ...&gt;</span></li>
<li><span class="math display">\displaystyle c\vec v</span> is a <strong>scalar multiple</strong> of <span class="math display">\displaystyle \vec v</span></li>
<li><span class="math display">\displaystyle c\vec v</span> and <span class="math display">\displaystyle \vec v</span> are said to be <strong>colinear</strong></li>
</ul>
<h3 id="dot-product">Dot product</h3>
<ul>
<li><span class="math display">\displaystyle \vec v \cdot \vec u = v_1 * u_1 + v_2 * u_2 + ... + v_n * u_n = |\vec v||\vec u|\cos(\theta) </span></li>
</ul>
<h3 id="cross-product">Cross Product</h3>
<ul>
<li>Cross product would result in a vector which is orthogonal to the 2 vectors</li>
<li><span class="math display">\displaystyle \vec v \times \vec u = </span>
<ol type="1">
<li><span class="math display">\displaystyle  |\vec v||\vec u|\sin(\theta)\hat{n} </span> where <span class="math display">\displaystyle \hat{n}</span> represents the normal vector to the plane constructed by u and v.</li>
<li><span class="math display">\displaystyle    \begin{pmatrix}
        u_2v_3 - u_3v_2\\ 
        u_3v_1 - u_1v_3\\ 
        u_1v_2 - u_2v_1\\ 
    \end{pmatrix} </span></li>
</ol></li>
<li>Unit vector cross product: <span class="math display">\displaystyle \hat i \times \hat j = 1 * 1 * \sin(90) * \hat{k} = \hat{k} </span>
<ul>
<li><span class="math display">\displaystyle  \hat i \times \hat i = 0 </span></li>
</ul></li>
<li>Cross product is not commutative, and it follows right-hand rule: cross product in counter clockwise direction would result in positive direction.
<ul>
<li>e.g. <span class="math display">\displaystyle  \hat i \times \hat j = \hat k </span>, <span class="math display">\displaystyle \hat j \times \hat i = -\hat k </span></li>
</ul></li>
<li><span class="math display">\displaystyle \tan{\theta} = \frac{|\vec u \times \vec v|}{\vec v \cdot \vec u} </span></li>
</ul>
<h2 id="linear-combinations">Linear Combinations</h2>
<ul>
<li>A linear combinations of vectors is a sum of scalar multiples of vectors.</li>
</ul>
<h3 id="linearly-independence">Linearly Independence</h3>
<ul>
<li>Definition
<ul>
<li>The vectors v1, v2,...., vk are said to be linearly independent if the only way that <span class="math display">\displaystyle a_1\vec v_1 + a_2\vec v_2 + ... a_k\vec v_k = 0 </span> can hold true is if 􏱘a1, a􏱘2, ..., a􏱘k are all zeroes.</li>
</ul></li>
<li>Characteristics
<ol type="1">
<li>A set of vectors are linearly independent iff no vector in M is a linear combination of others</li>
<li>If <span class="math display">\displaystyle \vec v </span> is non-zero, then {<span class="math display">\displaystyle \vec v</span>} is linearly independent</li>
</ol></li>
</ul>
<h2 id="orthogonal-perpendicular">Orthogonal (Perpendicular)</h2>
<ul>
<li>2 vectors are <strong>orthogonal</strong> if the dot product is 0 i.e. <span class="math display">\displaystyle  \vec x \cdot \vec y = \vec x^T * \vec y = 0 </span>
<ul>
<li>2 vectors are <strong>orthonormal</strong> if they are orthogonal and unit vectors</li>
</ul></li>
</ul>
<h2 id="projection">Projection</h2>
<ul>
<li>Projection of a on b = <span class="math display">\displaystyle  |\vec a| * cos(\theta) = \frac{\vec a\cdot \vec b}{|\vec b|} </span></li>
</ul>
<h3 id="height-distance">Height (distance)</h3>
<ul>
<li>Distance from point a to <span class="math display">\displaystyle \vec b = |\vec a| * sin(\theta) = \frac{|\vec v \times \vec u|}{|\vec b|} </span>
<ul>
<li>Area of triangle bounded by points a, b, c <span class="math display">\displaystyle  = \frac{|\vec{ab} \times \vec{ac}|}{|\vec{ac}|} * \vec{ac} / 2 = |\vec{ab} \times \vec{ac}| / 2 </span></li>
<li>Area of parallelogram bounded by points a, b, c, d <span class="math display">\displaystyle  = |\vec{ab} \times \vec{ac}| </span></li>
</ul></li>
</ul>
<h2 id="line">Line</h2>
<ul>
<li>A set of all vectors <span class="math display">\displaystyle \in \Re^3</span> in form <span class="math display">\displaystyle \vec v = \vec a + t\vec d</span> is referred to a line L containing vector <span class="math display">\displaystyle \vec a</span> and is parallel to <span class="math display">\displaystyle \vec d</span>
<ul>
<li><span class="math display">\displaystyle \vec v = \vec a + t\vec d</span> is the <strong>vector equation</strong> of line L</li>
<li><span class="math display">\displaystyle \vec d</span> is called the <strong>direction vector</strong> of line L</li>
<li>Line L will pass through point a and point (a + d)</li>
<li>Parametric equation of a line:
<ul>
<li><span class="math display">\displaystyle  \begin{aligned} 
v_1 &amp;= a_1 + t * d_1 \\
v_2 &amp;= a_2 + t * d_2 \\
v_3 &amp;= a_3 + t * d_3 \\
  \end{aligned} </span></li>
</ul></li>
<li>Cartesian equation of a line:
<ul>
<li><span class="math display">\displaystyle  t = \frac{v_1 - a_1}{d_1} = \frac{v_2 - a_2}{d_2} = \frac{v_3 - a_3}{d_3} </span></li>
</ul></li>
</ul></li>
<li>To find angle between 2 lines == angle between 2 direction vectors. Thus we can use dot product equations to get the angle</li>
<li>2 lines are parallel if <span class="math display">\displaystyle \vec d_1 </span> is a scalar multiple of <span class="math display">\displaystyle \vec d_2 </span></li>
</ul>
<h2 id="plane">Plane</h2>
<ul>
<li>Vector equations of a plane
<ol type="1">
<li><span class="math display">\displaystyle  \vec z = \vec b + s\vec v + t\vec w </span> where <span class="math display">\displaystyle \vec b</span> is a non-zero position vector, v and w are linearly independent</li>
<li><span class="math display">\displaystyle \vec n \cdot (\vec r - \vec a) = 0 </span> where n is normal, a is a specific point on plane (vector), and r is referring to any vectors on the plane (as dot product of orthogonal vectors is 0)
<ul>
<li>or <span class="math display">\displaystyle \vec n \cdot vec r = d </span>
<ul>
<li>If <span class="math display">\displaystyle \vec n</span> is a unit vector, then d is the distance from the plane to origin</li>
</ul></li>
</ul></li>
</ol></li>
<li>Cartesian equation
<ul>
<li><span class="math display">\displaystyle \begin{aligned} 
&amp;\vec n \cdot (\vec r - \vec a) = 0 \rarr \\ 
&amp;n_1 * (x - a_1) + n_2 * (y - a_2) + n_3 * (z - a_3) = 0 \rarr \\ 
&amp;n_1x + n_2y + n_3z = d  \\
\end{aligned} </span></li>
</ul></li>
</ul>
<h3 id="intersection-between-a-line-and-a-plane">Intersection between a line and a plane</h3>
<ul>
<li>Substitude the line's cartesian equation into the cartesian equation of a plane
<ul>
<li><span class="math display">\displaystyle  n_1 * (x - a_1) + n_2 * (y - a_2) + n_3 * (z - a_3) = 0  </span></li>
</ul></li>
</ul>
<h3 id="angle-of-intersection-between-2-planes">Angle of intersection between 2 planes</h3>
<ul>
<li>Equals to the angle between the normals of the 2 planes
<ul>
<li><span class="math display">\displaystyle \theta = cos^{-1}(\frac{\vec n_1 \cdot \vec n_2}{|\vec n_1||\vec n_2|}) </span></li>
</ul></li>
</ul>
<h3 id="hyperplane">Hyperplane</h3>
<h2 id="unit-vector">Unit vector</h2>
<ul>
<li>unit vector: vector with unit norms i.e. <span class="math display">\displaystyle  ||x|| = 1  </span></li>
</ul>
<h1 id="matrix-1">Matrix</h1>
<h2 id="overview">Overview</h2>
<ul>
<li><span class="math display">\displaystyle A \in \Re^{m*n} </span></li>
<li>Main diagonal
<ul>
<li>The diagonal line</li>
</ul></li>
<li>Transpose
<ul>
<li>The mirror image of the matrix across a diagonal line</li>
<li><span class="math display">\displaystyle  (A_{ij})^T = A_{ji} </span></li>
</ul></li>
<li>Matrix + Vector: <span class="math display">\displaystyle  C_{i,j} = A_{i,j} + b_j </span> (implicit copying of b to many locations is called broadcasting)</li>
</ul>
<h2 id="special-matrices">Special Matrices</h2>
<ul>
<li>Identity</li>
<li>Inverse: should not be calculated explicitly in practice</li>
<li>Diagonal</li>
<li>Symmetric</li>
<li>Orthogonal matrix: <span class="math display">\displaystyle  A^{-1} = A^T </span></li>
</ul>
<h2 id="operations-1">Operations</h2>
<h3 id="element-wise-product-hadamard-product">Element-wise product (Hadamard product)</h3>
<ul>
<li>element-wise product of 2 matrices</li>
<li><span class="math display">\displaystyle  C_{ij} = A_{ij} * A_{ij} </span></li>
<li>Denoted as <span class="math display">\displaystyle  A \odot B </span></li>
</ul>
<h3 id="matrices-multiplication">Matrices Multiplication</h3>
<ul>
<li><span class="math display">\displaystyle  A \in \Re^{m \times n}, B \in \Re^{n \times p}, C = AB \in \Re^{m \times p} </span>
<ul>
<li>complexity O(mnp)</li>
<li>Each entry of result C[i][j] is the dot product of ith row in A and jth column in B: <span class="math display">\displaystyle  C_{ij} = A_{i,:} \cdot B_{:,j} </span></li>
</ul></li>
<li>Properties
<ol type="1">
<li>Distributive: A(B+C) == AB + AC</li>
<li>Associative: A(BC) == (AB)C</li>
<li>NOT commutative: AB != BA<br />
</li>
</ol></li>
<li>Strassen's alg. <span class="math display">\displaystyle  O(n^{2.89}) </span></li>
</ul>
<h2 id="matrix-inverse">Matrix Inverse</h2>
<ul>
<li>A is invertable if and only if <span class="math display">\displaystyle  A \in \Re^{n*n} </span> and <span class="math display">\displaystyle  A^{-1}*A = A * A^{-1} = I </span></li>
<li>Thm (Sherman-Morrisen-Woodbury): <span class="math display">\displaystyle  A\in\Re^{n*n}, B\in\Re^{m*m}, U\in\Re^{n*m}, V\in\Re^{n*m}, (A + uBV)^{-1} = A^{-1} - A^{-1}U(B^{-1}+V^TA^{-1}U)^{-1}V^TA^{-1} </span>
<ul>
<li>In particular, <span class="math display">\displaystyle  (A + \vec u \vec v)^{-1} = A^{-1} - \frac{1}{1+v^TA^{-1}u}A^{-1}uv^TA^{-1} </span></li>
<li>Proof: verify that the product is 1</li>
</ul></li>
</ul>
<h2 id="determinant">Determinant</h2>
<h2 id="system-of-linear-equations">System of linear equations</h2>
<ul>
<li>Format: <span class="math display">\displaystyle  Ax = \vec b </span> where <span class="math display">\displaystyle  A \in \Re^{m*n}, x \in \Re^{n}, b \in \Re^{m} </span></li>
<li>Solution approaches
<ol type="1">
<li>Inverse: <span class="math display">\displaystyle  x = A^{-1}\vec b </span></li>
</ol></li>
<li>Pivot</li>
<li>Free variable</li>
<li><strong>rank</strong> of a matrix: maximum number of linearly independent column vectors</li>
</ul>
<h3 id="rref">RREF</h3>
<h3 id="gaussian-elimination">Gaussian Elimination</h3>
<h3 id="linear-dependence-and-span">Linear dependence and Span</h3>
<ul>
<li>Ax = b has a solution iff b is in the <strong>column space</strong> of A (span of A's column vectors)</li>
<li><strong>Linearly independent</strong> no vector in the set is a linear combination of other vectors</li>
</ul>
<h3 id="goal-solve-avec-x-vec-b-a-in-renn-a-is-invertible">Goal: solve <span class="math display">\displaystyle  A\vec x = \vec b, A \in \Re^{n*n} </span>, A is invertible</h3>
<ul>
<li><span class="math display">\displaystyle  \vec x = A^{-1}\vec b </span> is non-trivial for computer implementation</li>
<li>e.g. <span class="math display">\displaystyle    \begin{cases} 
            2x + y + 2z = 1 \\
            4x + 2y -z = 2 \\
            x + y + 2z = 3
        \end{cases} </span></li>
<li>to solve a linear system, try to transform the matrix <span class="math display">\displaystyle  \begin{bmatrix}
    2 &amp; 1 &amp; 2 \\ 
    4 &amp; 2 &amp; -1 \\
    1 &amp; 1 &amp; 2
\end{bmatrix} </span> to an upper triangular matrix</li>
<li>Psedo code for solving lower triangular matrix <span class="math display">\displaystyle  O(n^2) </span>
<ul>
<li><code>for n = 1, 2, ... n do</code><br />
<span class="math display">\displaystyle  y_i = (b_i - l^T_{i,1:(i-1)}y_{1:(i-1)})/l_{ii}; </span></li>
<li><strong>Problem</strong> division of <span class="math display">\displaystyle  l_{i,i} </span> could be 0.
<ul>
<li>solution: add a small number to all diagonals to avoid such issues / pivoting</li>
</ul></li>
</ul></li>
<li>During implementation, try to use vector instead of for-loop for calculation (avoid loop as much as possible)</li>
<li>Gaussian Elimination psedo code
<ul>
<li>Input: <span class="math display">\displaystyle  A \in \Re^{n*n}, \vec b \in \Re^{n*1} </span></li>
<li>output: in-place transform A to upper triangular matrix</li>
<li><p>code:</p>
<pre><code>for j = 1,2 ... n-1 do
    for i = j + 1, ..., n do</code></pre>
<pre><code>$$  a_{i,j} &lt;- a_{i,j} / a_{j,j} $$
$$  \vec a_{i,j+1:n} &lt;- \vec a_{i,j+1:n} - a_{ij} $$</code></pre></li>
<li>still have the problem of division
<ul>
<li>how to guarantee that the diagnals are never zero?</li>
<li>solution: compute the <strong>determinent</strong>: <span class="math display">\displaystyle  det(A) = \sum_{\sigma}(-1)^{sgn(\sigma)} * \prod^{n}_{i=1} A_{i, \sigma(i)} </span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="thm-lu-factorization">Thm (LU factorization)</h3>
<ul>
<li>let <span class="math display">\displaystyle \widetilde{L}</span> and u be the strict lower triangular and upper triangular part of the output of GE, then A = L*u, <span class="math display">\displaystyle  L = \widetilde{L} + I </span></li>
</ul>
<h3 id="to-solve-linear-system-ax-b">To solve linear system Ax = b</h3>
<ul>
<li>note: don't use inverse ever!</li>
<li><span class="math display">\displaystyle  \begin{aligned}
    A\vec x = \vec b, A &amp;= LU \\
    LU\vec x &amp;= \vec b \\
    L\vec y &amp;= \vec b, U\vec x = \vec y
\end{aligned} </span></li>
<li>Steps:
<ul>
<li>run GE on A and b</li>
<li>forward solve to get y from Ly = b</li>
<li>backward solve to get x from Ux = y</li>
</ul></li>
<li>time complexity is <span class="math display">\displaystyle  O(n^3) </span></li>
</ul>
<h2 id="eigen-decomposition">Eigen Decomposition</h2>
<ul>
<li><span class="math display">\displaystyle  Av = \lambda v </span> where v is an eigenvector and <span class="math display">\displaystyle \lambda </span> is an eigenvalue</li>
<li><strong>Characteristic equation</strong> <span class="math display">\displaystyle det(A - \lambda I) = 0 </span>
<ul>
<li><span class="math display">\displaystyle \lambda </span> is an eigenvalue of <span class="math display">\displaystyle A</span> iff it is a root of characteristic equation
<ul>
<li>proof:
<ul>
<li>if <span class="math display">\displaystyle  \lambda </span> is an eigenvalue, then <span class="math display">\displaystyle  (\lambda I - A)u = 0 </span> has a non-trivial solution</li>
<li>so <span class="math display">\displaystyle  \lambda I - A </span> is singular -&gt; <span class="math display">\displaystyle  det(A - \lambda I) = 0 </span> -&gt; <span class="math display">\displaystyle  \lambda </span> is a root</li>
</ul></li>
</ul></li>
</ul></li>
<li>Upon similarity transform, eigenvalues do not change</li>
<li>If <span class="math display">\displaystyle  A \in \Re^{n*n} </span> is a square matrix with rank n, then <span class="math display">\displaystyle  A = V \Lambda V^{-1} </span>, where <span class="math display">\displaystyle V_{:,i}</span> is ith eigenvector, and <span class="math display">\displaystyle  \Lambda_{i,i} </span> is ith eigenvalue</li>
</ul>
<h2 id="singular-value-decomposition">Singular Value Decomposition</h2>
<h1 id="references">References</h1>
<ul>
<li>MATH 136, University of Waterloo</li>
<li>CS 370, University of Waterloo</li>
<li>CS 475, Univeristy of Waterloo</li>
<li>https://www.deeplearningbook.org/contents/linear_algebra.html</li>
</ul>
<div class="status-banner" style="display: none; position: fixed; bottom: 0; left: 0; right: 0; text-align: center;">
    <div style="display: inline-block; padding: 0.8em 2em 0.5em 2em; background: black; color: white; font-size: 2em;">
        Rendering <svg xmlns="http://www.w3.org/2000/svg" height="1.4em" viewbox="0 0 1200 500" style="vertical-align: text-bottom"><title>LaTeX logo</title><g transform="matrix(45 0 0 45 40 40)" fill="white"><path d="M5.5 4.4C5.5 4.4 5.2 4.4 5.2 4.4 5.1 5.4 5 6.7 3.2 6.7 3.2 6.7 2.4 6.7 2.4 6.7 1.9 6.7 1.9 6.6 1.9 6.3 1.9 6.3 1.9 1 1.9 1 1.9 0.6 1.9 0.5 2.9 0.5 2.9 0.5 3.2 0.5 3.2 0.5 3.2 0.5 3.2 0.2 3.2 0.2 2.8 0.2 1.9 0.2 1.5 0.2 1.1 0.2 0.3 0.2 0 0.2 0 0.2 0 0.5 0 0.5 0 0.5 0.2 0.5 0.2 0.5 1 0.5 1 0.6 1 0.9 1 0.9 1 6.2 1 6.2 1 6.6 1 6.7 0.2 6.7 0.2 6.7 0 6.7 0 6.7 0 6.7 0 7 0 7 0 7 5.2 7 5.2 7 5.2 7 5.5 4.4 5.5 4.4z"/><path d="M5.3 0.2C5.3 0 5.2 0 5.1 0 5 0 4.9 0 4.9 0.2 4.9 0.2 3.3 4.2 3.3 4.2 3.2 4.4 3.1 4.7 2.5 4.7 2.5 4.7 2.5 5 2.5 5 2.5 5 4 5 4 5 4 5 4 4.7 4 4.7 3.7 4.7 3.5 4.6 3.5 4.4 3.5 4.3 3.5 4.3 3.6 4.2 3.6 4.2 3.9 3.4 3.9 3.4 3.9 3.4 5.9 3.4 5.9 3.4 5.9 3.4 6.3 4.4 6.3 4.4 6.3 4.4 6.3 4.5 6.3 4.5 6.3 4.7 5.9 4.7 5.8 4.7 5.8 4.7 5.8 5 5.8 5 5.8 5 7.7 5 7.7 5 7.7 5 7.7 4.7 7.7 4.7 7.7 4.7 7.6 4.7 7.6 4.7 7.1 4.7 7.1 4.7 7 4.5 7 4.5 5.3 0.2 5.3 0.2zM4.9 0.9C4.9 0.9 5.8 3.1 5.8 3.1 5.8 3.1 4 3.1 4 3.1 4 3.1 4.9 0.9 4.9 0.9z"/><path d="M13.3 0.2C13.3 0.2 7.2 0.2 7.2 0.2 7.2 0.2 7 2.5 7 2.5 7 2.5 7.3 2.5 7.3 2.5 7.4 0.9 7.6 0.5 9.1 0.5 9.3 0.5 9.5 0.5 9.6 0.6 9.8 0.6 9.8 0.7 9.8 0.9 9.8 0.9 9.8 6.2 9.8 6.2 9.8 6.5 9.8 6.7 8.8 6.7 8.8 6.7 8.4 6.7 8.4 6.7 8.4 6.7 8.4 7 8.4 7 8.8 6.9 9.8 6.9 10.3 6.9 10.7 6.9 11.7 6.9 12.2 7 12.2 7 12.2 6.7 12.2 6.7 12.2 6.7 11.8 6.7 11.8 6.7 10.7 6.7 10.7 6.5 10.7 6.2 10.7 6.2 10.7 0.9 10.7 0.9 10.7 0.7 10.7 0.6 10.9 0.6 11 0.5 11.3 0.5 11.5 0.5 13 0.5 13.1 0.9 13.2 2.5 13.2 2.5 13.5 2.5 13.5 2.5 13.5 2.5 13.3 0.2 13.3 0.2z"/><path d="M18.7 6.7C18.7 6.7 18.4 6.7 18.4 6.7 18.2 8.2 17.9 8.9 16.2 8.9 16.2 8.9 14.9 8.9 14.9 8.9 14.4 8.9 14.4 8.8 14.4 8.5 14.4 8.5 14.4 5.9 14.4 5.9 14.4 5.9 15.3 5.9 15.3 5.9 16.3 5.9 16.4 6.2 16.4 7 16.4 7 16.6 7 16.6 7 16.6 7 16.6 4.4 16.6 4.4 16.6 4.4 16.4 4.4 16.4 4.4 16.4 5.2 16.3 5.5 15.3 5.5 15.3 5.5 14.4 5.5 14.4 5.5 14.4 5.5 14.4 3.2 14.4 3.2 14.4 2.8 14.4 2.8 14.9 2.8 14.9 2.8 16.2 2.8 16.2 2.8 17.7 2.8 18 3.3 18.1 4.7 18.1 4.7 18.4 4.7 18.4 4.7 18.4 4.7 18.1 2.5 18.1 2.5 18.1 2.5 12.5 2.5 12.5 2.5 12.5 2.5 12.5 2.8 12.5 2.8 12.5 2.8 12.7 2.8 12.7 2.8 13.5 2.8 13.5 2.9 13.5 3.2 13.5 3.2 13.5 8.4 13.5 8.4 13.5 8.8 13.5 8.9 12.7 8.9 12.7 8.9 12.5 8.9 12.5 8.9 12.5 8.9 12.5 9.2 12.5 9.2 12.5 9.2 18.2 9.2 18.2 9.2 18.2 9.2 18.7 6.7 18.7 6.7z"/><path d="M21.7 3.1C21.7 3.1 23 1.1 23 1.1 23.3 0.8 23.6 0.5 24.5 0.5 24.5 0.5 24.5 0.2 24.5 0.2 24.5 0.2 22.1 0.2 22.1 0.2 22.1 0.2 22.1 0.5 22.1 0.5 22.5 0.5 22.7 0.7 22.7 0.9 22.7 1 22.7 1.1 22.6 1.2 22.6 1.2 21.5 2.8 21.5 2.8 21.5 2.8 20.2 0.9 20.2 0.9 20.2 0.9 20.1 0.8 20.1 0.8 20.1 0.7 20.4 0.5 20.8 0.5 20.8 0.5 20.8 0.2 20.8 0.2 20.4 0.2 19.7 0.2 19.3 0.2 19 0.2 18.4 0.2 18 0.2 18 0.2 18 0.5 18 0.5 18 0.5 18.2 0.5 18.2 0.5 18.8 0.5 19 0.5 19.2 0.8 19.2 0.8 21 3.6 21 3.6 21 3.6 19.4 6 19.4 6 19.2 6.2 18.9 6.7 17.9 6.7 17.9 6.7 17.9 7 17.9 7 17.9 7 20.3 7 20.3 7 20.3 7 20.3 6.7 20.3 6.7 19.8 6.7 19.7 6.4 19.7 6.2 19.7 6.1 19.7 6.1 19.8 6 19.8 6 21.2 3.9 21.2 3.9 21.2 3.9 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.3 22.8 6.4 22.8 6.5 22.6 6.7 22.2 6.7 22.2 6.7 22.2 7 22.2 7 22.5 6.9 23.2 6.9 23.6 6.9 24 6.9 24.5 7 24.9 7 24.9 7 24.9 6.7 24.9 6.7 24.9 6.7 24.7 6.7 24.7 6.7 24.2 6.7 24 6.6 23.8 6.3 23.8 6.3 21.7 3.1 21.7 3.1z"/></g></svg> math...
    </div>
</div>
<div class="license">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a> This work by <a xmlns:cc="http://creativecommons.org/ns#" href="https://uberi.github.io/" property="cc:attributionName" rel="cc:attributionURL">Jasper Wang</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  Copyright 2013-2021 Jasper Wang.
</div>
</body>
</html>
